# CS224N Assignment 2

## Understanding word2vec

### (a) Equivalency of naive-softmax and cross-entropy loss

$y_w = 0$ for $w \neq o$, and $y_w = 1$ for $w = o$, thus $-\sum_{w\in\textrm{vocab}}y_w\log(\hat{y}_w) = -\log(\hat{y}_o)$

### (b) Partial derivative of naive-softmax loss w.r.t. $v_c$

$$
\begin{aligned}
J_\textrm{naive-softmax}(v_c,o,U) &= -\log P(O=o|C=c) \\
&= -\log\frac{\exp(u_o^Tv_c)}{\sum_{w\in\textrm{vocab}}\exp(u_w^Tv_c)}
\end{aligned}
$$

$$
\frac{\partial J_\textrm{naive-softmax}(v_c,o,U)}{\partial v_c} = -\frac{\partial}{\partial v_c}u_o^Tv_c + \frac{\partial}{\partial v_c}\log\sum_{w\in\textrm{vocab}}\exp(u_w^Tv_c)
$$

Differentiating the second component of the equation above:

$$
\frac{\partial}{\partial v_c}\log\sum_{w\in\textrm{vocab}}\exp(u_w^Tv_c) = \frac{\partial y(u(v_c))}{\partial v_c} = \frac{\partial y}{\partial u}\frac{\partial u}{\partial v_c}
$$

where

$$
\begin{aligned}
u(v_c) &= \sum_{w\in\textrm{vocab}}\exp(u_w^Tv_c) \\
\frac{\partial u}{\partial v_c} &= \sum_{w\in\textrm{vocab}}u_w\exp(u_w^Tv_c)
\end{aligned}
$$

Thus

$$
\begin{aligned}
\frac{\partial J_\textrm{naive-softmax}(v_c,o,U)}{\partial v_c} &= -u_o + \sum_{w\in\textrm{vocab}}\frac{\exp(u_w^Tv_c)}{\sum_{w\in\textrm{vocab}}\exp(u_w^Tv_c)}\cdot u_w \\
&= -\sum_{w\in\textrm{vocab}}(y_w-\hat{y}_w)u_w \\
&= -(y-\hat{y})U
\end{aligned}
$$

### (c) Partial derivative of naive-softmax loss w.r.t. $u_w$

When $w = o$

$$
\begin{aligned}
\frac{\partial J_\textrm{naive-softmax}(v_c,o,U)}{\partial u_o} &= -\frac{\partial}{\partial u_o}u_o^Tv_c + \frac{\partial}{\partial u_o}\log\left[\sum_{w\in\textrm{vocab},w\neq o}\left[\exp(u_w^Tv_c)\right] + \exp(u_o^Tv_c)\right] \\
&= -v_c + v_c\frac{\exp(u_o^Tv_c)}{\sum_{w\in\textrm{vocab}}\exp(u_w^Tv_c)} \\
&= -v_c + v_c\hat{y}_o \\
&= v_c(\hat{y}_o - 1)
\end{aligned}
$$

When $w\neq o$

$$
\begin{aligned}
\frac{\partial J_\textrm{naive-softmax}(v_c,o,U)}{\partial u_w} &= -\frac{\partial}{\partial u_w}u_o^Tv_c + \frac{\partial}{\partial u_w}\log\left[\sum_{w\in\textrm{vocab},w\neq o}\left[\exp(u_w^Tv_c)\right] + \exp(u_o^Tv_c)\right] \\
&= v_c\frac{\exp(u_w^Tv_c)}{\sum_{w\in\textrm{vocab}}\exp(u_w^Tv_c)} \\
&= v_c\hat{y}_w
\end{aligned}
$$

### (d) Derivative of sigmoid function

$$
\begin{aligned}
\frac{d\sigma(x)}{dx} &= \frac{\exp(-x)}{(1 + \exp(-x))(1 + \exp(-x))} \\
&= \frac{1}{1 + \exp(-x)}\cdot\frac{\exp(-x)}{1 + \exp(-x)} \\
&= \frac{1}{1 + \exp(-x)}\cdot\frac{\exp(-x) + 1 - 1}{1 + \exp(-x)} \\
&= \frac{1}{1 + \exp(-x)}\cdot\left(\frac{1 + \exp(-x)}{1 + \exp(-x)} - \frac{1}{1 + \exp(-x)}\right) \\
&= \sigma(x)\left(1 - \sigma(x)\right)
\end{aligned}
$$

### (e) Partial derivatives of negative-sampling loss



### (e)