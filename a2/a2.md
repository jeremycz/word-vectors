# CS224N Assignment 2

## Understanding word2vec

### (a) Equivalency of naive-softmax and cross-entropy loss

$y_w = 0$ for $w \neq o$, and $y_w = 1$ for $w = o$, thus $-\sum_{w\in\textrm{vocab}}y_w\log(\hat{y}_w) = -\log(\hat{y}_o)$

### (b) Partial derivative of naive-softmax loss w.r.t. $v_c$

$$
\begin{aligned}
J_\textrm{naive-softmax}(v_c,o,U) &= -\log P(O=o|C=c) \\
&= -\log\frac{\exp(u_o^Tv_c)}{\sum_{w\in\textrm{vocab}}\exp(u_w^Tv_c)}
\end{aligned}
$$

$$
\frac{\partial J_\textrm{naive-softmax}(v_c,o,U)}{\partial v_c} = -\frac{\partial}{\partial v_c}u_o^Tv_c + \frac{\partial}{\partial v_c}\log\sum_{w\in\textrm{vocab}}\exp(u_w^Tv_c)
$$

Differentiating the second component of the equation above:

$$
\frac{\partial}{\partial v_c}\log\sum_{w\in\textrm{vocab}}\exp(u_w^Tv_c) = \frac{\partial y(u(v_c))}{\partial v_c} = \frac{\partial y}{\partial u}\frac{\partial u}{\partial v_c}
$$

where

$$
\begin{aligned}
u(v_c) &= \sum_{w\in\textrm{vocab}}\exp(u_w^Tv_c) \\
\frac{\partial u}{\partial v_c} &= \sum_{w\in\textrm{vocab}}u_w\exp(u_w^Tv_c)
\end{aligned}
$$

Thus

$$
\begin{aligned}
\frac{\partial J_\textrm{naive-softmax}(v_c,o,U)}{\partial v_c} &= -u_o + \sum_{w\in\textrm{vocab}}\frac{\exp(u_w^Tv_c)}{\sum_{w\in\textrm{vocab}}\exp(u_w^Tv_c)}\cdot u_w \\
&= -\sum_{w\in\textrm{vocab}}(y_w-\hat{y}_w)u_w \\
&= -(y-\hat{y})U
\end{aligned}
$$

### (c) Partial derivative of naive-softmax loss w.r.t. $u_w$

When $w = o$

$$
\begin{aligned}
\frac{\partial J_\textrm{naive-softmax}(v_c,o,U)}{\partial u_o} &= -\frac{\partial}{\partial u_o}u_o^Tv_c + \frac{\partial}{\partial u_o}\log\left[\sum_{w\in\textrm{vocab},w\neq o}\left[\exp(u_w^Tv_c)\right] + \exp(u_o^Tv_c)\right] \\
&= -v_c + v_c\frac{\exp(u_o^Tv_c)}{\sum_{w\in\textrm{vocab}}\exp(u_w^Tv_c)} \\
&= -v_c + v_c\hat{y}_o \\
&= v_c(\hat{y}_o - 1)
\end{aligned}
$$

When $w\neq o$

$$
\begin{aligned}
\frac{\partial J_\textrm{naive-softmax}(v_c,o,U)}{\partial u_w} &= -\frac{\partial}{\partial u_w}u_o^Tv_c + \frac{\partial}{\partial u_w}\log\left[\sum_{w\in\textrm{vocab},w\neq o}\left[\exp(u_w^Tv_c)\right] + \exp(u_o^Tv_c)\right] \\
&= v_c\frac{\exp(u_w^Tv_c)}{\sum_{w\in\textrm{vocab}}\exp(u_w^Tv_c)} \\
&= v_c\hat{y}_w
\end{aligned}
$$

### (d) Derivative of sigmoid function

$$
\begin{aligned}
\frac{d\sigma(x)}{dx} &= \frac{\exp(-x)}{(1 + \exp(-x))(1 + \exp(-x))} \\
&= \frac{1}{1 + \exp(-x)}\cdot\frac{\exp(-x)}{1 + \exp(-x)} \\
&= \frac{1}{1 + \exp(-x)}\cdot\frac{\exp(-x) + 1 - 1}{1 + \exp(-x)} \\
&= \frac{1}{1 + \exp(-x)}\cdot\left(\frac{1 + \exp(-x)}{1 + \exp(-x)} - \frac{1}{1 + \exp(-x)}\right) \\
&= \sigma(x)\left(1 - \sigma(x)\right)
\end{aligned}
$$

### (e) Partial derivatives of negative-sampling loss

Loss function:

$$
J_\textrm{neg-sample}(v_c,o,U) = -\log(\sigma(u_o^Tv_c))-\sum_{k=1}^K\log(\sigma(-u_k^Tv_c)), k\in[1,K], k \neq o
$$

Partial derivatives of individual terms in the loss function w.r.t. $v_c$, $u_o$ and $u_k$:

$$
\begin{aligned}
\frac{\partial}{\partial v_c}\log(\sigma(u_o^Tv_c)) &= \frac{\partial\log(u)}{\partial u}\frac{\partial\sigma(v)}{\partial v}\frac{\partial(u_o^Tv_c)}{\partial v_c}, u = \sigma(u_o^Tv_c), v = u_o^Tv_c \\
&= \frac{1}{\sigma(v)}\cdot\sigma(v)(1-\sigma(v))\cdot u_o \\
&= u_o\cdot(1-\sigma(u_o^Tv_c))
\end{aligned}
$$

$$
\frac{\partial}{\partial v_c}\sum_{k=1}^K\log(\sigma(-u_k^Tv_c)) = -\sum_{k=1}^Ku_k\cdot(1-\sigma(-u_k^Tv_c))
$$

$$
\frac{\partial}{\partial u_o}\log(\sigma(u_o^Tv_c)) = v_c\cdot(1-\sigma(u_o^Tv_c))
$$

$$
\frac{\partial}{\partial u_k}\sum_{k=1}^K\log(\sigma(-u_k^Tv_c)) = -v_c\cdot(1-\sigma(-u_k^Tv_c))
$$

Partial derivatives of the loss function w.r.t. $v_c$, $u_o$ and $u_k$:

$$
\begin{aligned}
\frac{\partial J_\textrm{neg-sample}(v_c,o,U)}{\partial v_c} &= -u_o\cdot(1-\sigma(u_o^Tv_c)) + \sum_{k=1}^Ku_k\cdot(1-\sigma(-u_k^Tv_c)) \\
\frac{\partial J_\textrm{neg-sample}(v_c,o,U)}{\partial u_o} &= -v_c\cdot(1-\sigma(u_o^Tv_c)) \\
\frac{\partial J_\textrm{neg-sample}(v_c,o,U)}{\partial u_k} &= v_c\cdot(1-\sigma(-u_k^Tv_c))
\end{aligned}
$$

Computing the negative-sampling loss is more efficient than the naive-softmax loss as the latter requires computing the softmax over the entire vocabulary, which may be on the order of a million words. In contrast, the former only samples a fixed number of negative samples, the number of which may be orders of magnitude lower than the size of the entire vocabulary.

### (f) Partial derivatives of skip-gram cost function

$$
\begin{aligned}
\frac{\partial J_\textrm{skip-gram}(v_c, w_{t-m},\dots,w_{t+m},U)}{\partial U} &= \sum_{-m\leq j\leq m, j \neq 0}\frac{\partial J(v_c, w_{t+j}, U)}{\partial U}\\
\frac{\partial J_\textrm{skip-gram}(v_c, w_{t-m},\dots,w_{t+m},U)}{\partial v_c} &= \sum_{-m\leq j\leq m, j \neq 0}\frac{\partial J(v_c, w_{t+j}, U)}{\partial v_c}\\
\frac{\partial J_\textrm{skip-gram}(v_c, w_{t-m},\dots,w_{t+m},U)}{\partial v_{w, w\neq c}} &= 0\\
\end{aligned}
$$
